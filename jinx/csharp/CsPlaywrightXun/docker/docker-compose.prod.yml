# Docker Compose override for Production environment
version: '3.8'

services:
  automation-framework:
    environment:
      - ASPNETCORE_ENVIRONMENT=Production
      - DOTNET_ENVIRONMENT=Production
      - HEADLESS=true
      - BROWSER_TYPE=chromium
      - LOG_LEVEL=Warning
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
        reservations:
          cpus: '2.0'
          memory: 4G
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    healthcheck:
      test: ["CMD", "dotnet", "--info"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Production database with high availability
  test-database:
    environment:
      POSTGRES_DB: testdata_prod
      POSTGRES_USER: produser
      POSTGRES_PASSWORD: ${PROD_DB_PASSWORD}
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G
    volumes:
      - postgres_prod_data:/var/lib/postgresql/data
      - ./backups:/backups
    command: >
      postgres
      -c shared_preload_libraries=pg_stat_statements
      -c pg_stat_statements.track=all
      -c max_connections=200
      -c shared_buffers=256MB
      -c effective_cache_size=1GB

  # Production load balancer with SSL
  nginx:
    image: nginx:alpine
    container_name: nginx-prod
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/prod.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
      - ./nginx/logs:/var/log/nginx
    depends_on:
      - automation-framework
    networks:
      - automation-network
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M

  # Redis cluster for production
  redis-cache:
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '0.5'
          memory: 1G
    command: redis-server --appendonly yes --cluster-enabled yes

  # Production monitoring stack
  prometheus:
    volumes:
      - ./monitoring/prometheus-prod.yml:/etc/prometheus/prometheus.yml
      - prometheus_prod_data:/prometheus
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
    
  grafana:
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      - GF_SECURITY_SECRET_KEY=${GRAFANA_SECRET_KEY}
      - GF_SMTP_ENABLED=true
      - GF_SMTP_HOST=${SMTP_HOST}
      - GF_SMTP_USER=${SMTP_USER}
      - GF_SMTP_PASSWORD=${SMTP_PASSWORD}
    volumes:
      - grafana_prod_data:/var/lib/grafana
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G

  # Production log management
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: elasticsearch-prod
    environment:
      - cluster.name=automation-cluster
      - node.name=es-node-1
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
      - xpack.security.enabled=true
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
    volumes:
      - elasticsearch_prod_data:/usr/share/elasticsearch/data
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
    networks:
      - automation-network

  # Backup service
  backup:
    image: postgres:15
    container_name: backup-service
    environment:
      - PGPASSWORD=${PROD_DB_PASSWORD}
    volumes:
      - ./backups:/backups
      - ./scripts/backup.sh:/backup.sh
    command: >
      sh -c "
        chmod +x /backup.sh &&
        crontab -l | { cat; echo '0 2 * * * /backup.sh'; } | crontab - &&
        crond -f
      "
    depends_on:
      - test-database
    networks:
      - automation-network

  # Security scanner
  security-scanner:
    image: aquasec/trivy:latest
    container_name: security-scanner
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./security-reports:/reports
    command: >
      sh -c "
        while true; do
          trivy image --format json --output /reports/scan-$(date +%Y%m%d).json automation-framework:latest
          sleep 86400
        done
      "
    networks:
      - automation-network

volumes:
  postgres_prod_data:
  prometheus_prod_data:
  grafana_prod_data:
  elasticsearch_prod_data: